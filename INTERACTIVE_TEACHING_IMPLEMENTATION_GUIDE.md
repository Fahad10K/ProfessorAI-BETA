# üéì Interactive Teaching Mode Implementation Guide
**Prof_AI-8126 - Two-Way Voice Teaching System**

**Date:** February 7, 2026  
**Status:** ‚úÖ Phase 1 COMPLETE - Server Implementation DONE | Phase 2 PENDING - Client Implementation  
**VAD Strategy:** Hybrid (Client-side noise gate + Server-side Deepgram VAD)  
**Reference Implementation:** AUM-ADMIN-B-Repo

---

## üéâ Phase 1 Implementation Summary

### ‚úÖ **Server-Side Implementation COMPLETED**

**Total Code Added:** ~470 lines  
**Files Modified:** `websocket_server.py`  
**Implementation Time:** ~2 hours

#### **New Methods Implemented:**

1. **`handle_interactive_teaching()`** - 193 lines
   - Initializes teaching session with STT service
   - Loads course content
   - Generates teaching material
   - Starts Deepgram STT for voice input
   - Falls back to one-way teaching if STT unavailable

2. **`_handle_teaching_interruptions()`** - 80 lines  
   - Listens for Deepgram VAD events
   - Cancels TTS on `speech_started` (barge-in)
   - Processes user questions on `final` transcript
   - Saves messages to PostgreSQL database

3. **`_stream_teaching_content()`** - 66 lines
   - Streams teaching audio with cancellation support
   - Checks interruption flag each chunk
   - Sends `teaching_interrupted` on cancel

4. **`_answer_teaching_question()`** - 126 lines
   - Retrieves conversation history (last 5 turns)
   - Calls chat service with course context
   - Streams answer audio
   - Saves Q&A to database

5. **`handle_stt_audio_chunk()`** - 20 lines
   - Receives audio chunks from client
   - Forwards PCM16 data to Deepgram

6. **`handle_continue_teaching()`** - 28 lines
   - Resumes teaching after Q&A

7. **`handle_end_teaching()`** - 33 lines
   - Closes STT service
   - Cancels TTS tasks
   - Cleanup resources

#### **Message Router Updated:**
- Added 4 new message type handlers
- `interactive_teaching`
- `stt_audio_chunk`
- `continue_teaching`
- `end_teaching`

---

## üîß What Works Now (Server-Side)

1. ‚úÖ **Session Creation** - User sessions persisted to PostgreSQL
2. ‚úÖ **Course Loading** - Modules and topics loaded correctly
3. ‚úÖ **Teaching Content Generation** - Via TeachingService
4. ‚úÖ **Deepgram STT Integration** - Real-time speech recognition
5. ‚úÖ **Barge-in Detection** - TTS cancels when user speaks
6. ‚úÖ **Question Processing** - ChatService with RAG + conversation history
7. ‚úÖ **Answer Audio Streaming** - ElevenLabs TTS with chunks
8. ‚úÖ **Database Persistence** - All messages saved with metadata
9. ‚úÖ **Error Handling** - Fallbacks and timeouts implemented
10. ‚úÖ **Resource Cleanup** - Proper STT/TTS task management

---

## ‚úÖ Phase 2 - Client Implementation COMPLETED

**File Created:** `interactive-teaching-client.html` (~850 lines)  
**Implementation Time:** ~1.5 hours

### **Features Implemented:**

#### 1. **Microphone Access & Audio Processing** ‚úÖ
- MediaDevices API for 16kHz mono audio capture
- Echo cancellation, noise suppression, AGC enabled
- ScriptProcessor for real-time audio processing

#### 2. **Client-Side VAD (Hybrid Layer 1)** ‚úÖ
- RMS calculation for voice activity detection
- Configurable threshold (0.01 default)
- Noise gate (0.005) to filter background noise
- Silence detection with 1200ms timeout
- Minimum speech duration filter (500ms)

#### 3. **Audio Streaming** ‚úÖ
- Float32 to PCM16 conversion
- Base64 encoding for WebSocket transmission
- Continuous streaming to Deepgram via server
- Automatic chunk batching (1024 samples)

#### 4. **WebSocket Message Handling** ‚úÖ
All server message types supported:
- `connection_ready` - Connection established
- `interactive_teaching_started` - Session begins
- `teaching_audio_chunk` - Streaming teaching audio
- `teaching_segment_complete` - Section finished
- `user_interrupt_detected` - Barge-in acknowledged
- `partial_transcript` - Interim STT results
- `user_question` - Question transcribed
- `teaching_question_answer` - Answer text
- `answer_audio_chunk` - Answer audio streaming
- `answer_complete` - Q&A cycle finished
- `teaching_interrupted` - TTS cancelled
- `teaching_resumed` - Lesson continues
- `teaching_ended` - Session cleanup
- `error` - Error handling

#### 5. **Audio Playback** ‚úÖ
- Queue-based audio management
- Sequential chunk playback
- Automatic URL cleanup
- Base64 to Audio element conversion

#### 6. **UI Components** ‚úÖ
- Real-time connection status indicators
- Microphone state visualization
- VAD level bar (animated)
- Conversation log with role-based styling
- Course/module/topic selectors
- Start/Continue/End controls
- System message notifications

#### 7. **User Experience** ‚úÖ
- Visual feedback for all states
- Smooth animations (pulse, slide-in)
- Responsive design
- Clear instructions
- Error handling with user-friendly messages

---

## üìã Table of Contents

1. [Current State Analysis](#current-state-analysis)
2. [Target Architecture](#target-architecture)
3. [Component Comparison](#component-comparison)
4. [Implementation Phases](#implementation-phases)
5. [Code Changes Required](#code-changes-required)
6. [Testing Strategy](#testing-strategy)
7. [Deployment Plan](#deployment-plan)

---

## üìä Current State Analysis

### ‚úÖ Already Implemented in Prof_AI-8126

#### 1. **Deepgram STT Service** ‚úÖ
**File:** `services/deepgram_stt_service.py`

**Status:** ALREADY EXISTS - Full Implementation
- ‚úÖ WebSocket connection to Deepgram Flux v2
- ‚úÖ Real-time streaming STT with VAD
- ‚úÖ TurnInfo event handling (StartOfTurn, EndOfTurn, EagerEndOfTurn)
- ‚úÖ Async queue-based event processing
- ‚úÖ Proper connection management and cleanup
- ‚úÖ Error handling and reconnection logic

**Key Features:**
```python
class DeepgramSTTService:
    - start() -> bool                    # Connect to Deepgram
    - recv() -> AsyncGenerator[dict]     # Yield STT events
    - send_audio_chunk(pcm16_bytes)      # Send audio to Deepgram
    - close()                            # Cleanup connection
    
    Events emitted:
    - {"type": "speech_started"}         # User starts speaking
    - {"type": "partial", "text": "..."} # Interim transcript
    - {"type": "final", "text": "..."}   # Final transcript
    - {"type": "utterance_end"}          # User stops speaking
```

**Config Support:**
- `DEEPGRAM_API_KEY` in config.py (line 14) ‚úÖ
- Environment variable support ‚úÖ

---

#### 2. **WebSocket Server Infrastructure** ‚úÖ
**File:** `websocket_server.py`

**Status:** PARTIALLY IMPLEMENTED

**Existing Components:**
- ‚úÖ `ProfAIWebSocketWrapper` - Connection management
- ‚úÖ `ProfAIAgent` - Service orchestration
- ‚úÖ Session management integration
- ‚úÖ Message routing system
- ‚úÖ Error handling and metrics

**Current Message Handlers:**
```python
- handle_ping()               # Heartbeat
- handle_chat_with_audio()    # Q&A with audio response
- handle_start_class()        # One-way teaching ‚ùå
- handle_audio_only()         # TTS only
- handle_transcribe_audio()   # STT only
- handle_set_language()       # Language switching
- handle_get_metrics()        # Performance metrics
```

**Session Management:**
- ‚úÖ SessionManager initialized (line 160)
- ‚úÖ Conversation history retrieval (limit=5)
- ‚úÖ Message persistence to PostgreSQL
- ‚úÖ User/session tracking with IP/user_agent

---

#### 3. **Audio Services** ‚úÖ
**File:** `services/audio_service.py`

**Status:** FULLY IMPLEMENTED

**Features:**
- ‚úÖ ElevenLabs TTS streaming
- ‚úÖ Deepgram STT integration
- ‚úÖ Multi-language support (11 languages)
- ‚úÖ Audio chunk streaming (<300ms latency)
- ‚úÖ Base64 encoding for WebSocket transmission

**Methods:**
```python
AudioService:
    - stream_audio_from_text()      # Streaming TTS
    - transcribe_audio_stream()     # Streaming STT
    - transcribe_audio()            # One-shot STT
```

---

#### 4. **Teaching Service** ‚úÖ
**File:** `services/teaching_service.py`

**Status:** EXISTS - Course content generation

**Purpose:** Generates teaching content from course materials
- ‚úÖ Content formatting for spoken delivery
- ‚úÖ Language support
- ‚úÖ LLM-based content enhancement

---

#### 5. **Chat Service with RAG** ‚úÖ
**File:** `services/chat_service.py`

**Status:** FULLY FUNCTIONAL

**Features:**
- ‚úÖ Semantic routing (greeting/general/course queries)
- ‚úÖ RAG pipeline with ChromaDB
- ‚úÖ Conversation history support
- ‚úÖ Course-specific filtering
- ‚úÖ Session-aware responses

---

### ‚ùå NOT Implemented (Gaps)

#### 1. **Interactive Teaching Handler** ‚ùå
**Missing:** Two-way teaching with interruption support

**Current `handle_start_class()` limitations:**
- ‚ùå One-way audio streaming only
- ‚ùå No user voice input during teaching
- ‚ùå Cannot interrupt or ask questions
- ‚ùå No barge-in/cancellation support
- ‚ùå Teaching continues regardless of user

**What's needed:**
- ‚úÖ Start STT streaming when teaching begins
- ‚úÖ Listen for user interruptions (speech_started)
- ‚úÖ Cancel current TTS when interrupted
- ‚úÖ Process user questions in teaching context
- ‚úÖ Resume teaching after Q&A

---

#### 2. **Barge-in Mechanism** ‚ùå
**Missing:** Ability to cancel ongoing TTS

**Current state:**
- `handle_start_class()` streams audio chunks
- No mechanism to stop mid-stream
- No task cancellation on user speech

**What's needed:**
- Store TTS task reference: `self.current_tts_task`
- Cancel task when `speech_started` event received
- Send `teaching_interrupted` message to client
- Track teaching state: `is_teaching` boolean

---

#### 3. **Teaching Session State** ‚ùå
**Missing:** Persistent teaching session tracking

**What's needed:**
```python
self.teaching_session = {
    'active': bool,
    'course_id': int,
    'module_index': int,
    'sub_topic_index': int,
    'teaching_content': str,
    'teaching_position': int,  # Resume point
    'is_teaching': bool,       # Agent speaking
    'current_tts_task': Task,  # Cancellable
    'stt_service': DeepgramSTTService,
    'conversation_context': list
}
```

---

#### 4. **Client-Side VAD & Audio Streaming** ‚ùå
**Missing:** Browser-based continuous microphone

**Current clients:**
- REST API clients (no WebSocket voice)
- Basic WebSocket test clients (no VAD)

**What's needed:** `interactive-teaching-client.html`
- Continuous microphone access
- Browser-based VAD (AudioWorklet)
- PCM16 audio chunk streaming
- Real-time audio playback
- Interruption UI feedback

---

## üèóÔ∏è Target Architecture

### System Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  BROWSER CLIENT                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Microphone   ‚îÇ‚Üí ‚îÇ     VAD      ‚îÇ‚Üí ‚îÇ PCM16 Stream ‚îÇ ‚îÇ
‚îÇ  ‚îÇ (16kHz mono) ‚îÇ  ‚îÇ  Detection   ‚îÇ  ‚îÇ   (chunks)   ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                              ‚îÇ         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ              WebSocket Messages          ‚îÇ        ‚îÇ‚îÇ
‚îÇ  ‚îÇ  {type: "stt_audio_chunk", audio: "..."}‚îÇ        ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                              ‚Üì         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                               ‚îÇ
                    WebSocket Connection (ws://host:8765)
                                               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PROF_AI-8126 SERVER             ‚Üì         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  websocket_server.py - Message Router          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - handle_interactive_teaching()                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - handle_stt_audio_chunk() [NEW]              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - handle_continue_teaching() [NEW]            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - handle_end_teaching() [NEW]                 ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                          ‚îÇ                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  TEACHING SESSION STATE‚îÇ                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - STT Service (Deepgram)               ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - TTS Task (cancellable)               ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - Course context                       ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ - Conversation history                 ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  EVENT FLOW                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  1. speech_started ‚Üí Cancel TTS                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  2. final transcript ‚Üí Process question         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  3. Chat service ‚Üí Generate answer              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  4. Stream answer audio                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  5. Answer complete ‚Üí Resume teaching           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  SERVICES                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - DeepgramSTTService (ALREADY EXISTS) ‚úÖ       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - AudioService (ElevenLabs TTS) ‚úÖ             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - ChatService (RAG + LLM) ‚úÖ                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - TeachingService (Content gen) ‚úÖ             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - SessionManager (DB persistence) ‚úÖ           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéôÔ∏è Hybrid VAD Architecture

### **Two-Layer VAD System** (Client + Server)

#### **Layer 1: Client-Side Noise Gate** (Browser JavaScript)
**Purpose:** Bandwidth optimization & instant UI feedback

**Implementation:**
```javascript
// Calculate RMS (Root Mean Square) of audio
const VAD_THRESHOLD = 0.01;  // Configurable sensitivity
let sum = 0;
for (let i = 0; i < inputData.length; i++) {
    sum += inputData[i] * inputData[i];
}
const rms = Math.sqrt(sum / inputData.length);

// Only send audio chunks when RMS exceeds threshold
if (rms > VAD_THRESHOLD) {
    sendAudioChunk(pcm16Data);  // Send to server
    updateUIIndicator(rms);     // Visual feedback
}
```

**Benefits:**
- ‚úÖ Reduces bandwidth by ~70% (don't send silence)
- ‚úÖ Instant visual feedback (no server round-trip)
- ‚úÖ Lower Deepgram API costs

**Limitations:**
- ‚ö†Ô∏è Not used for interruption detection
- ‚ö†Ô∏è Simple threshold, may miss quiet speech

---

#### **Layer 2: Server-Side Deepgram VAD** (Primary Decision Maker)
**Purpose:** Professional ML-based turn-taking detection

**Implementation:**
```python
# Deepgram sends TurnInfo events via WebSocket
async for event in stt_service.recv():
    if event['type'] == 'speech_started':
        # OFFICIAL: User started speaking
        # ‚Üí Cancel TTS (barge-in)
        cancel_current_teaching_audio()
    
    elif event['type'] == 'final':
        # OFFICIAL: User finished speaking
        # ‚Üí Process question
        process_user_question(event['text'])
```

**Benefits:**
- ‚úÖ Professional ML-based VAD (Deepgram Nova-3)
- ‚úÖ Reliable turn-taking detection
- ‚úÖ Handles background noise, accents, variations
- ‚úÖ **Used for all critical decisions** (barge-in, transcription)

---

### **Flow: How Both VADs Work Together**

```
User speaks ‚Üí Client RMS > threshold ‚Üí Send audio chunks
                                              ‚Üì
                                    Deepgram receives audio
                                              ‚Üì
                                    ML-based VAD analysis
                                              ‚Üì
                         TurnInfo: StartOfTurn event emitted
                                              ‚Üì
                              Server receives speech_started
                                              ‚Üì
                                  Cancel TTS (barge-in)
                                              ‚Üì
                              Notify client (UI update)
```

**Key Point:** Client VAD is a **filter**, server VAD is the **decision maker**.

---

## üîÑ Component Comparison

### AUM-ADMIN-B vs Prof_AI-8126

| Component | AUM Implementation | Prof_AI-8126 Status | Action Needed |
|-----------|-------------------|---------------------|---------------|
| **STT Service** | `deepgram_stt_service.py` | ‚úÖ EXISTS (identical) | ‚úÖ None |
| **TTS Service** | `elevenlabs_direct_service.py` | ‚úÖ `audio_service.py` | ‚úÖ Compatible |
| **WebSocket Server** | `run_simple_audio_server.py` | ‚úÖ `websocket_server.py` | ‚ö†Ô∏è Add handlers |
| **Barge-in Logic** | Lines 326-343 in server | ‚ùå Not implemented | ‚ùå Implement |
| **Teaching Context** | N/A (general chat) | ‚úÖ `teaching_service.py` | ‚úÖ Integrate |
| **Session Mgmt** | In-memory history | ‚úÖ PostgreSQL | ‚úÖ Superior |
| **Course Data** | N/A | ‚úÖ `courses_week_topics.json` | ‚úÖ Advantage |
| **Client VAD** | `avatar-audio-client.html` | ‚ùå Not implemented | ‚ùå Create |
| **Conversation History** | Simple list | ‚úÖ DB with limit=5 | ‚úÖ Better |

---

## üìù Implementation Phases

### Phase 1: Server-Side Handlers (Week 1)

#### 1.1 Add `handle_interactive_teaching()` 
**File:** `websocket_server.py`

**Purpose:** Initialize two-way teaching session

**Pseudo-code:**
```python
async def handle_interactive_teaching(self, data: dict):
    """
    Start interactive teaching with voice interruption support.
    
    Flow:
    1. Load course content (existing logic from handle_start_class)
    2. Generate teaching content (existing TeachingService)
    3. Initialize Deepgram STT service
    4. Start STT event listener (background task)
    5. Begin streaming teaching audio (cancellable task)
    """
    
    # Extract parameters
    course_id = data.get("course_id")
    module_index = data.get("module_index", 0)
    sub_topic_index = data.get("sub_topic_index", 0)
    language = data.get("language", "en-IN")
    
    # Initialize teaching session state
    self.teaching_session = {
        'active': True,
        'course_id': course_id,
        'module_index': module_index,
        'sub_topic_index': sub_topic_index,
        'teaching_content': None,
        'is_teaching': False,
        'current_tts_task': None,
        'stt_service': None,
        'user_id': data.get("user_id") or f"ws_{self.client_id}",
        'conversation_context': []
    }
    
    # Load course content (COPY from handle_start_class lines 595-653)
    # Generate teaching content (COPY from handle_start_class lines 655-720)
    
    # NEW: Start STT service
    from services.deepgram_stt_service import DeepgramSTTService
    stt_service = DeepgramSTTService(sample_rate=16000)
    stt_started = await stt_service.start()
    
    if not stt_started:
        await self.websocket.send({
            "type": "stt_unavailable",
            "message": "Voice input not available, using one-way teaching"
        })
        # Fallback to regular start_class
        return await self.handle_start_class(data)
    
    self.teaching_session['stt_service'] = stt_service
    
    # NEW: Start STT event listener
    asyncio.create_task(self._handle_teaching_interruptions())
    
    # Send ready message
    await self.websocket.send({
        "type": "interactive_teaching_started",
        "module_title": module['title'],
        "sub_topic_title": sub_topic['title'],
        "message": "Interactive teaching ready. Speak anytime to ask questions!"
    })
    
    # Start teaching audio (cancellable)
    await self._stream_teaching_content(teaching_content, language)
```

**Estimated Time:** 4-6 hours

---

#### 1.2 Add `_handle_teaching_interruptions()`
**File:** `websocket_server.py`

**Purpose:** Listen for user speech and handle interruptions

**Reference:** AUM `run_simple_audio_server.py` lines 321-438

**Implementation:**
```python
async def _handle_teaching_interruptions(self):
    """
    Background task that listens to STT events during teaching.
    
    Handles:
    - speech_started: Cancel TTS, notify client
    - final transcript: Process user question
    - utterance_end: Ready for next action
    """
    
    stt_service = self.teaching_session.get('stt_service')
    if not stt_service:
        return
    
    try:
        async for event in stt_service.recv():
            event_type = event.get('type')
            
            if event_type == 'speech_started':
                # USER INTERRUPTION DETECTED
                log(f"üó£Ô∏è User interrupted teaching for client {self.client_id}")
                
                # Stop current teaching audio
                if self.teaching_session.get('current_tts_task'):
                    self.teaching_session['current_tts_task'].cancel()
                    self.teaching_session['is_teaching'] = False
                    log("‚èπÔ∏è Cancelled teaching TTS")
                
                # Notify client
                await self.websocket.send({
                    "type": "user_interrupt_detected",
                    "message": "Listening to your question..."
                })
            
            elif event_type == 'final':
                # User finished speaking - got full question
                user_question = event.get('text', '').strip()
                if not user_question:
                    continue
                
                log(f"üìù User question: {user_question}")
                
                # Echo transcript
                await self.websocket.send({
                    "type": "user_question",
                    "text": user_question
                })
                
                # Save to database
                if self.session_manager and self.session_id:
                    self.session_manager.add_message(
                        user_id=self.teaching_session['user_id'],
                        session_id=self.session_id,
                        role='user',
                        content=user_question,
                        message_type='voice',
                        course_id=self.teaching_session['course_id']
                    )
                
                # Answer the question
                await self._answer_teaching_question(user_question)
            
            elif event_type == 'utterance_end':
                log("üîá User stopped speaking")
                # Already handled by final event
                pass
    
    except Exception as e:
        log(f"Error in teaching interruption handler: {e}")
```

**Estimated Time:** 3-4 hours

---

#### 1.3 Add `_stream_teaching_content()`
**File:** `websocket_server.py`

**Purpose:** Stream teaching audio with cancellation support

**Implementation:**
```python
async def _stream_teaching_content(self, content: str, language: str):
    """
    Stream teaching content audio, allowing interruption.
    
    Difference from handle_start_class:
    - Stores task reference for cancellation
    - Checks is_teaching flag each chunk
    - Sends teaching_interrupted on cancel
    """
    
    try:
        self.teaching_session['is_teaching'] = True
        
        async def send_teaching_audio():
            chunk_count = 0
            
            # Stream audio chunks
            async for audio_chunk in self.audio_service.stream_audio_from_text(
                content, language, self.websocket
            ):
                # CHECK IF INTERRUPTED
                if not self.teaching_session.get('is_teaching', False):
                    log("üõë Teaching interrupted by user")
                    await self.websocket.send({"type": "teaching_interrupted"})
                    return
                
                # Send chunk
                chunk_count += 1
                audio_base64 = base64.b64encode(audio_chunk).decode('utf-8')
                await self.websocket.send({
                    "type": "teaching_audio_chunk",
                    "chunk_id": chunk_count,
                    "audio_data": audio_base64
                })
            
            # Teaching segment complete
            await self.websocket.send({
                "type": "teaching_segment_complete",
                "message": "This section is complete. Ask questions or continue?"
            })
            
            self.teaching_session['is_teaching'] = False
        
        # Create cancellable task
        self.teaching_session['current_tts_task'] = asyncio.create_task(send_teaching_audio())
        
        # Wait for completion or cancellation
        await self.teaching_session['current_tts_task']
        
    except asyncio.CancelledError:
        log("Teaching TTS cancelled by user")
        await self.websocket.send({"type": "teaching_cancelled"})
    except Exception as e:
        log(f"Error streaming teaching: {e}")
```

**Estimated Time:** 2-3 hours

---

#### 1.4 Add `_answer_teaching_question()`
**File:** `websocket_server.py`

**Purpose:** Answer user question in teaching context

**Implementation:**
```python
async def _answer_teaching_question(self, question: str):
    """
    Answer user's question using chat service with teaching context.
    
    Context includes:
    - Current teaching content
    - Course ID for filtering
    - Conversation history from DB
    """
    
    try:
        # Get conversation history (last 5 turns)
        conversation_history = []
        if self.session_id:
            conversation_history = self.session_manager.get_conversation_history(
                self.session_id, limit=5
            )
        
        # Call chat service (existing - with course_id filter)
        response_data = await self.chat_service.ask_question(
            question,
            language=self.current_language,
            session_id=self.session_id,
            conversation_history=conversation_history,
            course_id=self.teaching_session['course_id']
        )
        
        answer_text = response_data.get('answer', '')
        
        # Send text response
        await self.websocket.send({
            "type": "teaching_question_answer",
            "text": answer_text,
            "route": response_data.get('route'),
            "sources": response_data.get('sources', [])
        })
        
        # Save assistant message
        if self.session_manager and self.session_id:
            self.session_manager.add_message(
                user_id=self.teaching_session['user_id'],
                session_id=self.session_id,
                role='assistant',
                content=answer_text,
                message_type='voice',
                course_id=self.teaching_session['course_id'],
                metadata={
                    'route': response_data.get('route'),
                    'sources': response_data.get('sources')
                }
            )
        
        # Stream answer audio
        self.teaching_session['is_teaching'] = True
        chunk_count = 0
        
        async for audio_chunk in self.audio_service.stream_audio_from_text(
            answer_text, self.current_language, self.websocket
        ):
            chunk_count += 1
            audio_base64 = base64.b64encode(audio_chunk).decode('utf-8')
            await self.websocket.send({
                "type": "answer_audio_chunk",
                "chunk_id": chunk_count,
                "audio_data": audio_base64
            })
        
        # Answer complete
        await self.websocket.send({
            "type": "answer_complete",
            "message": "Would you like to continue the lesson?"
        })
        
        self.teaching_session['is_teaching'] = False
        
    except Exception as e:
        log(f"Error answering teaching question: {e}")
        await self.websocket.send({
            "type": "error",
            "error": "Failed to answer question"
        })
```

**Estimated Time:** 3-4 hours

---

#### 1.5 Add `handle_stt_audio_chunk()`
**File:** `websocket_server.py`

**Purpose:** Forward audio chunks from client to STT service

**Implementation:**
```python
async def handle_stt_audio_chunk(self, data: dict):
    """
    Receive audio chunk from client and forward to Deepgram.
    
    Message format:
    {
        "type": "stt_audio_chunk",
        "audio": "<base64 PCM16 data>"
    }
    """
    
    # Check if teaching session active with STT
    if not self.teaching_session or not self.teaching_session.get('stt_service'):
        return
    
    audio_base64 = data.get('audio')
    if not audio_base64:
        return
    
    try:
        # Decode base64 to PCM16 bytes
        pcm_bytes = base64.b64decode(audio_base64)
        
        # Send to Deepgram
        await self.teaching_session['stt_service'].send_audio_chunk(pcm_bytes)
        
    except Exception as e:
        log(f"Error forwarding audio to STT: {e}")
```

**Estimated Time:** 1 hour

---

#### 1.6 Update Message Router
**File:** `websocket_server.py` (lines 245-262)

**Add new message types:**
```python
elif message_type == "interactive_teaching":
    await self.handle_interactive_teaching(data)

elif message_type == "stt_audio_chunk":
    await self.handle_stt_audio_chunk(data)

elif message_type == "continue_teaching":
    await self.handle_continue_teaching(data)

elif message_type == "end_teaching":
    await self.handle_end_teaching(data)
```

**Estimated Time:** 30 minutes

---

### Phase 2: Client-Side Implementation (Week 2)

#### 2.1 Create `interactive-teaching-client.html`

**Base Template:** Copy from AUM `avatar-audio-client.html`

**Key Sections to Adapt:**

**A. Microphone + VAD Setup (lines 599-752 from AUM)**
```javascript
// Continuous microphone with Voice Activity Detection
async function startAudioProcessing() {
    audioContext = new AudioContext({ sampleRate: 16000 });
    const source = audioContext.createMediaStreamSource(mediaStream);
    const processor = audioContext.createScriptProcessor(1024, 1, 1);
    
    // VAD configuration
    const VAD_THRESHOLD = 0.015;
    const VAD_SILENCE_DURATION = 900;
    const MIN_SPEECH_DURATION = 400;
    
    processor.onaudioprocess = (e) => {
        const inputData = e.inputBuffer.getChannelData(0);
        
        // Calculate RMS for VAD
        let sum = 0;
        for (let i = 0; i < inputData.length; i++) {
            sum += inputData[i] * inputData[i];
        }
        const rms = Math.sqrt(sum / inputData.length);
        
        // Voice Activity Detection
        if (rms > VAD_THRESHOLD) {
            if (!isSpeaking) {
                isSpeaking = true;
                onUserStartSpeaking();
            }
            
            // Send audio chunk to server
            const pcm16 = float32ToPCM16(inputData);
            sendAudioChunk(pcm16);
        } else if (isSpeaking) {
            // Silence detected
            setTimeout(() => {
                isSpeaking = false;
                onUserStopSpeaking();
            }, VAD_SILENCE_DURATION);
        }
    };
}
```

**B. WebSocket Message Handling**
```javascript
websocket.onmessage = (event) => {
    const data = JSON.parse(event.data);
    
    switch(data.type) {
        case 'interactive_teaching_started':
            showTeachingUI(data);
            break;
            
        case 'teaching_audio_chunk':
            playAudioChunk(data.audio_data);
            break;
            
        case 'user_interrupt_detected':
            stopTeachingAudio();
            showListeningIndicator();
            break;
            
        case 'user_question':
            displayUserQuestion(data.text);
            break;
            
        case 'teaching_question_answer':
            displayAnswer(data.text);
            break;
            
        case 'answer_audio_chunk':
            playAudioChunk(data.audio_data);
            break;
            
        case 'teaching_segment_complete':
            showContinuePrompt();
            break;
    }
};
```

**Estimated Time:** 8-10 hours

---

### Phase 3: Testing & Integration (Week 3)

#### 3.1 Unit Tests
- Test STT event handling
- Test TTS cancellation
- Test teaching session state management

#### 3.2 Integration Tests
- End-to-end teaching flow
- Interruption scenarios
- Resume teaching after Q&A
- Session persistence

#### 3.3 Performance Tests
- Latency measurements
- Concurrent user handling
- Memory leak checks

**Estimated Time:** 12-15 hours

---

## üìÅ Files to Modify

### Server Files

| File | Changes Required | Estimated Lines | Priority |
|------|-----------------|-----------------|----------|
| `websocket_server.py` | Add 5 new methods | +300 lines | üî¥ Critical |
| `config.py` | Verify DEEPGRAM_API_KEY | 0 lines (already exists) | ‚úÖ Done |
| `services/deepgram_stt_service.py` | None | 0 lines | ‚úÖ Done |

### Client Files

| File | Changes Required | Estimated Lines | Priority |
|------|-----------------|-----------------|----------|
| `interactive-teaching-client.html` | New file | +1000 lines | üî¥ Critical |

### Documentation Files

| File | Changes Required | Priority |
|------|-----------------|----------|
| `README.md` | Add interactive teaching section | üü° Medium |
| `API_DOCUMENTATION.md` | Document new message types | üü° Medium |

---

## üß™ Testing Strategy

### Test Scenarios

#### Scenario 1: Normal Teaching Flow
1. Client connects
2. Starts interactive teaching
3. Teaching audio streams
4. Teaching completes
5. Client disconnects

**Expected:** All audio chunks delivered, no errors

---

#### Scenario 2: User Interruption
1. Teaching in progress
2. User speaks (speech_started)
3. TTS cancelled
4. User question received
5. Answer provided
6. Teaching resumes

**Expected:** 
- TTS stops within 500ms of speech
- User question transcribed accurately
- Answer contextually relevant
- Teaching resumes from correct point

---

#### Scenario 3: Multiple Interruptions
1. Teaching starts
2. User interrupts (question 1)
3. Answer provided
4. User interrupts again (question 2)
5. Answer provided
6. Continue teaching

**Expected:** Each Q&A cycle handled correctly

---

#### Scenario 4: Session Persistence
1. User asks 3 questions
2. Check database for message records
3. Verify conversation history retrieval

**Expected:** All messages saved with correct metadata

---

## üöÄ Deployment Checklist

### Pre-Deployment

- [ ] All unit tests passing
- [ ] Integration tests completed
- [ ] Performance benchmarks met (<500ms latency)
- [ ] Documentation updated
- [ ] Code review completed

### Deployment Steps

1. **Backup current websocket_server.py**
   ```bash
   cp websocket_server.py websocket_server.py.backup
   ```

2. **Deploy updated server code**
   ```bash
   scp -i prof-ai.pem websocket_server.py root@103.150.187.86:~/Prof_AI-8126/
   ```

3. **Verify DEEPGRAM_API_KEY on server**
   ```bash
   ssh -i prof-ai.pem root@103.150.187.86
   grep DEEPGRAM_API_KEY ~/Prof_AI-8126/.env
   ```

4. **Rebuild Docker containers**
   ```bash
   cd ~/Prof_AI-8126
   docker-compose -f docker-compose-production.yml down
   docker-compose -f docker-compose-production.yml build --no-cache
   docker-compose -f docker-compose-production.yml up -d
   ```

5. **Monitor logs**
   ```bash
   docker-compose -f docker-compose-production.yml logs -f --tail=100
   ```

6. **Test with client**
   - Open `interactive-teaching-client.html`
   - Start teaching session
   - Test interruption
   - Verify conversation history

### Post-Deployment

- [ ] Monitor error rates
- [ ] Check latency metrics
- [ ] Verify database message storage
- [ ] User acceptance testing

---

## üìä Success Metrics

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| **Interruption Response Time** | <500ms | Time from speech_started to TTS cancel |
| **STT Accuracy** | >95% | Manual transcript review |
| **Answer Relevance** | >90% | User satisfaction survey |
| **System Uptime** | >99.9% | Monitoring logs |
| **Concurrent Users** | 50+ | Load testing |
| **Message Persistence** | 100% | Database audit |

---

## üîß Configuration Reference

### Environment Variables Required

```bash
# Already in .env
DEEPGRAM_API_KEY=your_deepgram_key_here
ELEVENLABS_API_KEY=your_elevenlabs_key_here
OPENAI_API_KEY=your_openai_key_here
DATABASE_URL=postgresql://...
REDIS_URL=rediss://...

# Optional (has defaults)
ELEVENLABS_VOICE_ID=21m00Tcm4TlvDq8ikWAM
```

### WebSocket Message Types

#### Client ‚Üí Server
```json
{
  "type": "interactive_teaching",
  "course_id": 1,
  "module_index": 0,
  "sub_topic_index": 0,
  "language": "en-IN",
  "user_id": "167"
}

{
  "type": "stt_audio_chunk",
  "audio": "<base64 PCM16 data>"
}

{
  "type": "continue_teaching"
}

{
  "type": "end_teaching"
}
```

#### Server ‚Üí Client
```json
{
  "type": "interactive_teaching_started",
  "module_title": "...",
  "sub_topic_title": "..."
}

{
  "type": "teaching_audio_chunk",
  "chunk_id": 1,
  "audio_data": "<base64>"
}

{
  "type": "user_interrupt_detected",
  "message": "Listening..."
}

{
  "type": "user_question",
  "text": "What is...?"
}

{
  "type": "teaching_question_answer",
  "text": "The answer is...",
  "route": "course_query",
  "sources": [...]
}

{
  "type": "answer_audio_chunk",
  "chunk_id": 1,
  "audio_data": "<base64>"
}

{
  "type": "teaching_segment_complete",
  "message": "Section complete"
}
```

---

## üéØ Next Steps

### Immediate Actions (Today)

1. ‚úÖ Review this document thoroughly
2. ‚úÖ Verify Deepgram API key is in .env
3. ‚úÖ Test existing `deepgram_stt_service.py` independently
4. ‚úÖ Create backup of `websocket_server.py`

### Week 1: Server Implementation

1. Implement `handle_interactive_teaching()` (Day 1-2)
2. Implement `_handle_teaching_interruptions()` (Day 2-3)
3. Implement `_stream_teaching_content()` (Day 3-4)
4. Implement `_answer_teaching_question()` (Day 4-5)
5. Add message routing (Day 5)

### Week 2: Client Implementation

1. Create `interactive-teaching-client.html` (Day 1-3)
2. Implement VAD and audio streaming (Day 3-4)
3. Add UI for interruption feedback (Day 5)

### Week 3: Testing & Polish

1. Unit tests (Day 1-2)
2. Integration tests (Day 3-4)
3. Performance optimization (Day 5)

---

## üìö Reference Links

### AUM Implementation Files (Reference)
- `AUM-ADMIN-B-Repo/Prof_AI/run_simple_audio_server.py` - Server logic
- `AUM-ADMIN-B-Repo/Prof_AI/services/deepgram_stt_service.py` - STT service
- `AUM-ADMIN-B-Repo/Prof_AI/websocket_tests/avatar-audio-client.html` - Client VAD

### Prof_AI-8126 Files (To Modify)
- `Prof_AI-8126/websocket_server.py` - Main modifications
- `Prof_AI-8126/services/deepgram_stt_service.py` - Already exists ‚úÖ
- `Prof_AI-8126/config.py` - Config verification

### Documentation
- Deepgram API: https://developers.deepgram.com/
- ElevenLabs API: https://elevenlabs.io/docs/
- WebSocket Protocol: https://developer.mozilla.org/en-US/docs/Web/API/WebSocket

---

**Document Version:** 1.0  
**Last Updated:** February 7, 2026  
**Status:** Ready for Implementation ‚úÖ

---

## üí° Key Insights

1. **Deepgram STT service already exists** - No need to create from scratch
2. **Most infrastructure is ready** - Session management, audio services, chat service all functional
3. **Main gap is handler logic** - Need to add ~300 lines of code to websocket_server.py
4. **Client is biggest effort** - Need to build full HTML/JS client with VAD
5. **Database persistence works** - Messages will be saved automatically with existing session_manager

**Total Estimated Time:** 3 weeks (120 hours)
- Week 1: Server (40 hours)
- Week 2: Client (40 hours)
- Week 3: Testing (40 hours)

**Ready to proceed with implementation? Start with Phase 1.1!** üöÄ
